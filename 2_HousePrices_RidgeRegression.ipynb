{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "<a id = \"ridge\"></a>\n",
    "In coding Ridge Regression you will be asked to:  \n",
    "- Mean center target variable and mean center / standardize observation  \n",
    "- Calculate Ridge Regression weights using linear algebra\n",
    "- Create a  hyperparameter tuning process   \n",
    "\n",
    "**Motivation**: Ridge Regression offers a way to mitigate some of the weaknesses of Least Squares Linear Regression to build more robust models.  \n",
    "\n",
    "Data remains the same as the one used in Kaggle House Prices Dataset (link provided in first project) \n",
    "\n",
    "<a id = \"code_ridge\"></a>\n",
    "### Coding Ridge Regression   \n",
    "\n",
    "#### Preprocessing\n",
    "Before implementing Ridge Regression, it is important to mean-center our target variable and mean-center and standardize observations. We will do this according to the following:  \n",
    "#### Mean Center Target\n",
    "$$y_{cent} = y_0 - \\bar{y}$$\n",
    "\n",
    "#### Standardize Observations\n",
    "$$X_{std} = \\frac{X_0-\\bar{X}}{s_{X}}$$\n",
    "\n",
    "Where $\\bar{X}$ is the sample mean of X and $s_{X}$ is the sample standard deviation of X.  \n",
    "\n",
    "NOTE: The sample standard deviation should be calculated with 0 \"Delta Degrees of Freedom\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This cell imports the necessary modules and sets a few plotting parameters for display\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (20.0, 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_path='train.csv'\n",
    "test_path='test.csv'\n",
    "data=pd.read_csv(tr_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code a function called \"standardize\" \n",
    "### ACCEPT one input, a list of numbers\n",
    "### RETURN a list where those values have been standardized.\n",
    "\n",
    "### To standardize, subtract the mean of the list and divide by standard deviation.\n",
    "### Use np.std for calculating standard deviation\n",
    "\n",
    "def standardize( num_list):\n",
    "    \"\"\"\n",
    "    Standardize the given list of numbers\n",
    "    \n",
    "    Positional arguments:\n",
    "        num_list -- a list of numbers\n",
    "    \n",
    "    Example:\n",
    "        num_list = [1,2,3,3,4,4,5,5,5,5,5]\n",
    "        nl_std = standardize(num_list)\n",
    "        print(np.round(nl_std,2))\n",
    "        #--> np.array([-2.11, -1.36, -0.61, -0.61,  \n",
    "                0.14,  0.14,  0.88,  0.88,  0.88,\n",
    "                0.88,  0.88])\n",
    "    \n",
    "    NOTE: the sample standard deviation should be calculated with 0 \"Delta Degrees of Freedom\"\n",
    "    \"\"\"\n",
    "    new_list=[]\n",
    "    \n",
    "    for num in num_list: \n",
    "        x=(num-np.mean(num_list))/np.std(num_list)\n",
    "        new_list.append(x)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Below we will create a function which will preprocess our data according to the functions above by performing:\n",
    "* mean subtraction from $y$,\n",
    "* dimension standardization for $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_regularization(data, y_column_name, x_column_names):\n",
    "    \"\"\"\n",
    "    Perform mean subtraction and dimension standardization on data\n",
    "        \n",
    "    Positional argument:\n",
    "        data -- a pandas dataframe of the data to pre-process\n",
    "        y_column_name -- the name (string) of the column that contains\n",
    "            the target of the training data.\n",
    "        x_column_names -- a *list* of the names of columns that contain the\n",
    "            observations to be standardized\n",
    "        \n",
    "    Returns:\n",
    "        Return a DataFrame consisting only of the columns included\n",
    "        in `y_column_name` and `x_column_names`.\n",
    "        Where the y_column has been mean-centered, and the\n",
    "        x_columns have been mean-centered/standardized.\n",
    "        \n",
    "        \n",
    "    Example:\n",
    "        data = pd.read_csv(tr_path).head()\n",
    "        prepro_data = preprocess_for_regularization(data,'SalePrice', ['GrLivArea','YearBuilt'])\n",
    "        \n",
    "        print(prepro_data) #-->\n",
    "                   GrLivArea  YearBuilt  SalePrice\n",
    "                0  -0.082772   0.716753     7800.0\n",
    "                1  -1.590161  -0.089594   -19200.0\n",
    "                2   0.172946   0.657024    22800.0\n",
    "                3  -0.059219  -1.911342   -60700.0\n",
    "                4   1.559205   0.627159    49300.0\n",
    "    \n",
    "    NOTE: The sample standard deviation should be calculated with 0 \"Delta Degrees of Freedom\"\n",
    "    \n",
    "    If your answer does not match the example answer,\n",
    "    check the default degrees of freedom in your standard deviation function.\n",
    "    \"\"\"\n",
    "    # Create list of all columns\n",
    "    toRetCol = x_column_names + [y_column_name]\n",
    "    \n",
    "    # subset dataframe\n",
    "    toRet = data[toRetCol].copy() # For \"setting with copy\" warning\n",
    "    \n",
    "    # calculate mean of y, then subtract from all y's\n",
    "    y_mean = np.mean(data[y_column_name])\n",
    "    \n",
    "    toRet[y_column_name] = data[y_column_name].apply(lambda x: x - y_mean)\n",
    "    \n",
    "    # Calc mean and std for every column in x\n",
    "    # Then apply standardization\n",
    "    for column in x_column_names:\n",
    "        mean = np.mean(data[column])\n",
    "        std = np.std(data[column])\n",
    "        \n",
    "        toRet[column] = data[column].apply(lambda x: (x - mean)/std)\n",
    "    \n",
    "    return toRet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you'll implement the equation for ridge regression using the closed form equation:  \n",
    "\n",
    "$$w_{RR}=(\\lambda+X^TX)^{-1}X^Ty$$  \n",
    "\n",
    "The function will be very similar to the function you wrote for Least Squares Regression with a slightly different matrix to invert.  \n",
    "\n",
    "NB: Many `numpy` matrix functions will be useful. e.g. `np.matmul`, `np.linalg.inv`, `np.ones`, `np.transpose`, and`np.identity`.\n",
    "\n",
    "The main change from Least Squares Regression is that $\\lambda$ is a parameter *we* must set. This is different from the $w$ parameters that we calculate from either closed form or approximation algorithms.  \n",
    "\n",
    "We will address tuning parameters such as $\\lambda$ in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code a function called \"ridge_regression_weights\"\n",
    "### ACCEPT three inputs:\n",
    "### Two matricies corresponding to the x inputs and y target\n",
    "### and a number (int or float) for the lambda parameter\n",
    "\n",
    "### RETURN a numpy array of regression weights\n",
    "\n",
    "### The following must be accomplished:\n",
    "\n",
    "### Ensure the number of rows of each the X matrix is greater than the number of columns.\n",
    "### ### If not, transpose the matrix.\n",
    "### Ultimately, the y input will have length n.\n",
    "### Thus the x input should be in the shape n-by-p\n",
    "\n",
    "### *Prepend* an n-by-1 column of ones to the input_x matrix\n",
    "\n",
    "### Use the above equation to calculate the least squares weights.\n",
    "### This will involve creating the lambda matrix---\n",
    "### ### a p+1-by-p+1 matrix with the \"lambda_param\" on the diagonal\n",
    "### ### p+1-by-p+1 because of the prepended \"ones\".\n",
    "\n",
    "def ridge_regression_weights(input_x, output_y, lambda_param):\n",
    "    \"\"\"Calculate ridge regression least squares weights.\n",
    "    \n",
    "    Positional arguments:\n",
    "        input_x -- 2-d matrix of input data\n",
    "        output_y -- 1-d numpy array of target values\n",
    "        lambda_param -- lambda parameter that controls how heavily\n",
    "            to penalize large weight values\n",
    "        \n",
    "    Example:\n",
    "        training_y = np.array([208500, 181500, 223500, \n",
    "                                140000, 250000, 143000, \n",
    "                                307000, 200000, 129900, \n",
    "                                118000])\n",
    "                                \n",
    "        training_x = np.array([[1710, 1262, 1786, \n",
    "                                1717, 2198, 1362, \n",
    "                                1694, 2090, 1774, \n",
    "                                1077], \n",
    "                               [2003, 1976, 2001, \n",
    "                                1915, 2000, 1993, \n",
    "                                2004, 1973, 1931, \n",
    "                                1939]])\n",
    "        lambda_param = 10\n",
    "        \n",
    "        rrw = ridge_regression_weights(training_x, training_y, lambda_param)\n",
    "        \n",
    "        print(rrw) #--> np.array([-576.67947107,   77.45913349,   31.50189177])\n",
    "        print(rrw[2]) #--> 31.50189177\n",
    "        \n",
    "    Assumptions:\n",
    "        -- output_y is a vector whose length is the same as the\n",
    "        number of observations in input_x\n",
    "        -- lambda_param has a value greater than 0\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check to ensure dataframe is long not wide\n",
    "    if input_x.shape[0] < input_x.shape[1]:\n",
    "        input_x = np.transpose(input_x)\n",
    "        \n",
    "    # Create column of ones\n",
    "    ones = np.ones((len(output_y), 1), dtype=int)\n",
    "    \n",
    "    # Add column of ones to X\n",
    "    augmented_x = np.concatenate((ones, input_x), axis=1)\n",
    "    \n",
    "    # Create square lambda_matrix, with size equal to number of columns in X\n",
    "    lambda_matrix = lambda_param * np.identity(min(augmented_x.shape))\n",
    "    \n",
    "    # Invert lambda + dot-prod of x and transposed x\n",
    "    inv = np.linalg.inv(lambda_matrix + np.matmul(np.transpose(augmented_x), augmented_x))\n",
    "    \n",
    "    # dot-prod of inverted matrix and transposed X\n",
    "    left_multiplier = np.matmul(inv , np.transpose(augmented_x))\n",
    "    \n",
    "    # final dot-prod with the ys\n",
    "    weights = np.matmul(left_multiplier, output_y)\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting the $\\lambda$ parameter\n",
    "\n",
    "For our final function before looking at the `sklearn` implementation of ridge regression, we will create a hyper-parameter tuning algorithm.  \n",
    "\n",
    "In ridge regression, we must pick a value for $\\lambda$. We have some intuition about $\\lambda$ from the equations that define it: small values tend to emulate the results from Least Squares, while large values will reduce the dimensionality of the problem. But the choice of $\\lambda$ can motivated with a more precise quantitative treatment.\n",
    "\n",
    "Eventually, we will look to choose the value of $\\lambda$ that minimizes validation error, which we will determine using $k$-fold cross-validation.\n",
    "\n",
    "For this example here, we will solve a simpler problem: Find a value that minimizes the of the list returned by a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example of hiden function below:\n",
    "\n",
    "### `hidden` takes a single number as a parameter (int or float) and returns a list of 1000 numbers\n",
    "### the input must be between 0 and 50 exclusive\n",
    "\n",
    "def hidden(hp):\n",
    "    if (hp<=0) or (hp >= 50):\n",
    "        print(\"input out of bounds\")\n",
    "    \n",
    "    nums = np.logspace(0,5,num = 1000)\n",
    "    vals = nums** 43.123985172351235134687934\n",
    "    \n",
    "    user_vals = nums** hp\n",
    "    \n",
    "    return vals-user_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the above cell and test out the functionality of `hidden`. Remember it takes a single number, between 0 and 50, as an argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code a function called \"minimize\"\n",
    "### ACCEPT one input: a function.\n",
    "\n",
    "### That function will be similar to `hidden` created above and available for your exploration.\n",
    "### Like 'hidden', the passed function will take a single argument, a number between 0 and 50 exclusive \n",
    "### and then, the function will return a numpy array of 1000 numbers.\n",
    "\n",
    "### RETURN the value that makes the mean of the array returned by 'passed_func' as close to 0 as possible\n",
    "\n",
    "### Note, you will almost certainly NOT be able to find the number that makes the mean exactly 0\n",
    "\n",
    "\n",
    "def minimize( passed_func):\n",
    "    \"\"\"\n",
    "    Find the numeric value that makes the mean of the\n",
    "    output array returned from 'passed_func' as close to 0 as possible.\n",
    "    \n",
    "    Positional Argument:\n",
    "        passed_func -- a function that takes a single number (between 0 and 50 exclusive)\n",
    "            as input, and returns a list of 1000 floats.\n",
    "        \n",
    "    Example:\n",
    "        passed_func = hidden\n",
    "        min_hidden = minimize(passed_func)\n",
    "        print(round(min_hidden,4))\n",
    "        #--> 43.1204 (answers will vary slightly, must be close to 43.123985172351)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Create values to test\n",
    "    test_vals = list(np.linspace(.1,49.9, 1000))\n",
    "\n",
    "    # Find mean of returned array from function\n",
    "    ret_vals = [abs(np.mean(passed_func(x))) for x in test_vals]\n",
    "\n",
    "    # Find smallest mean\n",
    "    min_mean = min(ret_vals)\n",
    "\n",
    "    # Return the test value that creates the smallest mean\n",
    "    return test_vals[ret_vals.index(min_mean)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above simulates hyper parameter tuning.  \n",
    "\n",
    "In the case of ridge regression, you would be searching lambda parameters to minimize validation error.  \n",
    "\n",
    "The `hidden` function would be analogous to the model building; the returned list analogous to residuals; and the mean of that list analogous to validation error.  \n",
    "\n",
    "See below for an example of using the functions built above that automatically performs hyper-parameter tuning using mean-absolute-deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9414414414414414"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lambda_search_func(lambda_param):\n",
    "    \n",
    "    # Define X and y\n",
    "    # with preprocessing\n",
    "    df = preprocess_for_regularization(data.head(50),'SalePrice', ['GrLivArea','YearBuilt'])\n",
    "    \n",
    "    y_true = df['SalePrice'].values\n",
    "    X = df[['GrLivArea','YearBuilt']].values\n",
    "    \n",
    "    # Calculate Weights then use for predictions\n",
    "    weights = ridge_regression_weights(X, y_true, lambda_param )\n",
    "    y_pred = weights[0] + np.matmul(X,weights[1:])\n",
    "    \n",
    "    # Calculate Residuals\n",
    "    resid = y_true - y_pred\n",
    "    \n",
    "    # take absolute value to tune on mean-absolute-deviation\n",
    "    # Alternatively, could use:\n",
    "    # return resid **2-S\n",
    "    # for tuning on mean-squared-error\n",
    "    \n",
    "    return abs(resid)\n",
    "\n",
    "minimize(lambda_search_func)    # --> about 1.4957957957957957"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"sklearn\"></a>\n",
    "\n",
    "### Ridge Regression in `sklearn`  \n",
    "\n",
    "Below gives the syntax for implementing ridge regression in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeastSquares Intercept: -2024976.362585636 Coefs: [  95.16733349 1045.86241944] \n",
      "\n",
      "Ridge alpha = 100000 Intercept: -1876820.7548806793 Coefs: [ 96.01060155 970.05661101] \n",
      "\n",
      "Ridge, alpha = 0 Intercept: -2024976.3625856352 Coefs: [  95.16733349 1045.86241944] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "\n",
    "### Note, the \"alpha\" parameter defines regularization strength.\n",
    "### Lambda is a reserved word in `Python` -- Thus \"alpha\" instead\n",
    "\n",
    "### An alpha of 0 is equivalent to least-squares regression\n",
    "lr = LinearRegression()\n",
    "reg = Ridge(alpha = 100000)\n",
    "reg0 = Ridge(alpha = 0)\n",
    "\n",
    "# Notice how the consistent sklearn syntax may be used to easily fit many kinds of models\n",
    "for m, name in zip([lr, reg, reg0], [\"LeastSquares\",\"Ridge alpha = 100000\",\"Ridge, alpha = 0\"]):\n",
    "    \n",
    "    m.fit(data[['GrLivArea','YearBuilt']], data['SalePrice'])\n",
    "    print(name, \"Intercept:\", m.intercept_, \"Coefs:\",m.coef_,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, in the above example, an alpha of 100,000 is set for the ridge regularization. The reason an alpha value this high is required is because standardization / mean centering of our inputs did not occur, and instead of working with inputs on the order of [-4,4] we are on the interval of [0,2000].  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
